{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77b9a4dd",
   "metadata": {},
   "source": [
    "![Opendoor](https://raw.githubusercontent.com/interviewquery/takehomes/opendoor_1/opendoor_1/logo.png)\n",
    "# Opendoor Data Science Take Home Problem Set\n",
    "\n",
    "The questions below are meant to give candidates a sense of the problems\n",
    "that we tackle at Opendoor. We expect solutions in the form of a\n",
    "jupyter notebook + working code. The problem set should take around 3 hours to\n",
    "complete.\n",
    "\n",
    "# Part 1. Simple housing model \n",
    "\n",
    "Your task is to implement a simple model to predict home prices for a\n",
    "small real estate transactions dataset.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1.  Load the data in `data.csv`\n",
    "\n",
    "2.  Predict the close price as of list date given the other attributes.\n",
    "\n",
    "3.  Build separate models with and without ListPrice.\n",
    "\n",
    "4.  Feel free to join the dataset to any other data sources, so long as\n",
    "    they are not leaky.\n",
    "\n",
    "## Questions\n",
    "\n",
    "1.  Describe your methodology. Why did you pick it?\n",
    "\n",
    "2.  How well would you do if you excluded the list price?\n",
    "\n",
    "3.  What is the performance of your model? What error metrics did you\n",
    "    choose?\n",
    "\n",
    "4.  How would you improve your model?\n",
    "\n",
    "5.  How would you host your model in a production environment to predict\n",
    "    values of homes in real-time?\n",
    "\n",
    "# Part 2. Simple ​$n$-grams\n",
    "\n",
    "Generate the top 10 trigrams for the article\n",
    "[http://en.wikipedia.org/wiki/N-gram](http://en.wikipedia.org/wiki/N-gram)\n",
    "\n",
    "# Part 3. Memoizer \n",
    "\n",
    "Write a function that accepts a single-argument function ​`f`​, and an\n",
    "integer `k`​ ​, and returns a function that behaves the same as ​`f`​ except\n",
    "it caches the last ​`k`​ distinct accessed results of ​`f`​.\n",
    "\n",
    "For instance, if ​`memoize`​ is the function we're after, and let ​`mem_f =\n",
    "memoize(f, 2)`, then\n",
    "\n",
    "-   `mem_f(arg1)​` → `f(arg1)​` ​ is computed and cached\n",
    "\n",
    "-   `mem_f(arg1)` → `f(arg1)` is returned from cache\n",
    "\n",
    "-   `mem_f(arg2)` → `f(arg2)` ​is computed and cached\n",
    "\n",
    "-   `mem_f(arg3)` → `f(arg3)` ​is computed and cached, and ​`f(arg1)​` is\n",
    "    evicted\n",
    "\n",
    "## Additional questions\n",
    "\n",
    "-   Can you describe the efficiency of the memoizer?\n",
    "\n",
    "-   How does your memoizer handle concurrent access?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c47429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data\n",
      "done reading, now parsing data\n",
      "creating a subset of 5000 random homes to run predictions on\n",
      "on prediction 0\n",
      "on prediction 50\n",
      "on prediction 100\n",
      "on prediction 150\n",
      "on prediction 200\n",
      "on prediction 250\n",
      "on prediction 300\n",
      "on prediction 350\n",
      "on prediction 400\n",
      "on prediction 450\n",
      "on prediction 500\n",
      "on prediction 550\n",
      "on prediction 600\n",
      "on prediction 650\n",
      "on prediction 700\n",
      "on prediction 750\n",
      "on prediction 800\n",
      "on prediction 850\n",
      "on prediction 900\n",
      "on prediction 950\n",
      "on prediction 1000\n",
      "on prediction 1050\n",
      "on prediction 1100\n",
      "on prediction 1150\n",
      "on prediction 1200\n",
      "on prediction 1250\n",
      "on prediction 1300\n",
      "on prediction 1350\n",
      "on prediction 1400\n",
      "on prediction 1450\n",
      "on prediction 1500\n",
      "on prediction 1550\n",
      "on prediction 1600\n",
      "on prediction 1650\n",
      "on prediction 1700\n",
      "on prediction 1750\n",
      "on prediction 1800\n",
      "on prediction 1850\n",
      "on prediction 1900\n",
      "on prediction 1950\n",
      "on prediction 2000\n",
      "on prediction 2050\n",
      "on prediction 2100\n",
      "on prediction 2150\n",
      "on prediction 2200\n",
      "on prediction 2250\n",
      "on prediction 2300\n",
      "on prediction 2350\n",
      "on prediction 2400\n",
      "on prediction 2450\n",
      "on prediction 2500\n",
      "on prediction 2550\n",
      "on prediction 2600\n",
      "on prediction 2650\n",
      "on prediction 2700\n",
      "on prediction 2750\n",
      "on prediction 2800\n",
      "on prediction 2850\n",
      "on prediction 2900\n",
      "on prediction 2950\n",
      "on prediction 3000\n",
      "on prediction 3050\n",
      "on prediction 3100\n",
      "on prediction 3150\n",
      "on prediction 3200\n",
      "on prediction 3250\n",
      "on prediction 3300\n",
      "on prediction 3350\n",
      "on prediction 3400\n",
      "on prediction 3450\n",
      "on prediction 3500\n",
      "on prediction 3550\n",
      "on prediction 3600\n",
      "on prediction 3650\n",
      "on prediction 3700\n",
      "on prediction 3750\n",
      "on prediction 3800\n",
      "on prediction 3850\n",
      "on prediction 3900\n",
      "on prediction 3950\n",
      "on prediction 4000\n",
      "on prediction 4050\n",
      "on prediction 4100\n",
      "on prediction 4150\n",
      "on prediction 4200\n",
      "on prediction 4250\n",
      "on prediction 4300\n",
      "on prediction 4350\n",
      "on prediction 4400\n",
      "on prediction 4450\n",
      "on prediction 4500\n",
      "on prediction 4550\n",
      "on prediction 4600\n",
      "on prediction 4650\n",
      "on prediction 4700\n",
      "on prediction 4750\n",
      "on prediction 4800\n",
      "on prediction 4850\n",
      "on prediction 4900\n",
      "on prediction 4950\n",
      "done with 5000 predictions!\n",
      "Median relative absolute error is 0.22613138876737865\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "import heapq\n",
    "from statistics import median\n",
    "from functools import total_ordering\n",
    "\n",
    "@total_ordering\n",
    "class Home(object):\n",
    "\tdef __init__(self, lat, lng, close, price):\n",
    "\t\tself.lat = float(lat)\n",
    "\t\tself.long = float(lng)\n",
    "\t\tself.close_date = parse(close) # convert to dateutil object for date comparison\n",
    "\t\tself.close_price = float(price)\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\treturn \"lat: {} long: {} close_date: {} close_price: {}\".format(self.lat, self.long, self.close_date, self.close_price)\n",
    "\n",
    "\tdef __eq__(self, other):\n",
    "\t\t# homes are equal if they have same lat, lng, close price, close date\n",
    "\t\treturn self.lat == other.lat and self.long == other.long and self.close_date == other.close_date and self.close_price == other.close_price\n",
    "\n",
    "\tdef __lt__(self, other):\n",
    "\t\treturn self.close_price < other.close_price\n",
    "\n",
    "def distance(home1, home2):\n",
    "\t# returns the l2 distance\n",
    "\treturn np.sqrt((home1.lat - home2.lat)**2 + (home1.long - home2.long)**2)\n",
    "\n",
    "def softmax(v):\n",
    "        \"\"\"Calculates the softmax function that outputs a vector of values that sum to one.\n",
    "        Softmax is np.exp(v) / sum(np.exp(v)), but this could present numerical overflow issues if v is very large. \n",
    "        We can work around this by adding a constant into the exponent. \n",
    "        This is because Ce^t = e^(t + logC)\n",
    "        \"\"\"\n",
    "        v = np.array(v)\n",
    "        logC = -np.max(v)\n",
    "        return np.exp(v + logC)/np.sum(np.exp(v + logC), axis = 0)\n",
    "\n",
    "\n",
    "def weight_homes(target_home, other_homes):\n",
    "\t# assigns weights to homes based on the target homes. \n",
    "\t# takes the distances from target homes as the weights, but then runs softmax over the resulting vector to normalize to one. \n",
    "\t# with more data about the homes, we would want to use a similarity_score(home1, home2) function here, instead of only using the l2 distance.\n",
    "\t# For example, if two homes are approximately the same distance to the target home, but also has the same square footage,\n",
    "\t# then it should have a larger weight.\n",
    "\treturn softmax([distance(target_home, home) for home in other_homes])\n",
    "\n",
    "def is_valid_home(current_home, candidate_home):\n",
    "\t# the candidate home must not equal the current home. It also needs to to have a strictly earlier close date. \n",
    "\treturn candidate_home != current_home and candidate_home.close_date < current_home.close_date\n",
    "\n",
    "def get_max_distance_candidate(current_home, candidate_homes):\n",
    "\treturn max(candidate_homes, key = lambda home: distance(current_home, home))\n",
    "\n",
    "def single_predict(current_home, data, k = 4):\n",
    "\tcandidate_homes = []\n",
    "\tfor d in data:\n",
    "\t\tif is_valid_home(current_home, d):\n",
    "\t\t\tif len(candidate_homes) < k:\n",
    "\t\t\t\tcandidate_homes.append(d)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcmax = get_max_distance_candidate(current_home, candidate_homes)\n",
    "\t\t\t\tif distance(current_home, d) < distance(current_home, cmax):\n",
    "\t\t\t\t\tcandidate_homes.remove(cmax)\n",
    "\t\t\t\t\tcandidate_homes.append(d)\n",
    "\t# sanity check: if a home is valid and not in the list of candidate homes, then its distance must be >= the maximum candidate home's distance to the current home.\n",
    "\tfor d in data:\n",
    "\t\tassert not is_valid_home(current_home, d) or d in candidate_homes \\\n",
    "\t\t\tor distance(d, current_home) >= distance(get_max_distance_candidate(current_home, candidate_homes), current_home), \\\n",
    "\t\t\t\"Sanity check failed, found home with a smaller distance diff that is not in the list.\"\n",
    "\t# sanity check: the candidate home should not include the current home, and all home close dates should be less than the current home's sold date\n",
    "\tif len(candidate_homes) == 0:\n",
    "\t\t# case where we picked the home with no prior sales before it - don't have any information to go off of\n",
    "\t\treturn 0\n",
    "\t# note: finding k neighbors may be impossible in case there are < k houses in the dataset that closed before it. \n",
    "\t# in this case, the price predicted is 0 if it is the single house with no houses sold before it. \n",
    "\t# otherwise, we use the n nearest neighbors to predict, but we have n < k. \n",
    "\tfor c in candidate_homes:\n",
    "\t\tassert c != current_home and c.close_date < current_home.close_date, \"Error: invalid home in candidate homes\"\n",
    "\t# weight the homes according to their distance to the current home\n",
    "\tweights = weight_homes(current_home, candidate_homes)\n",
    "\t# get the price of the current home\n",
    "\tprice = sum([weights[i] * candidate_homes[i].close_price for i in range(len(candidate_homes))])\n",
    "\treturn price\n",
    "\n",
    "def mrae(predicted_prices, actual_prices):\n",
    "\terrs = sorted([abs(predicted_prices[i] - actual_prices[i])/actual_prices[i]\n",
    "\t\tfor i in range(len(predicted_prices))])\n",
    "\treturn median(errs)\n",
    "\n",
    "\t\n",
    "if __name__ == '__main__':\n",
    "\tprint(\"reading data\")\n",
    "\twith open('data.csv') as f:\n",
    "\t\tlines = f.readlines()\n",
    "\tprint(\"done reading, now parsing data\")\n",
    "\thomes = []\n",
    "\tfor line in lines[1:]:\n",
    "\t\tlat, lng, close, price = line.strip().split(\",\")\n",
    "\t\th = Home(lat, lng, close, price)\n",
    "\t\thomes.append(h)\n",
    "\tprint(\"creating a subset of 5000 random homes to run predictions on\")\n",
    "\t# take a subset of size 5000 random homes\n",
    "\trandom_homes = [homes[np.random.randint(0, len(homes))] for _ in range(5000)]\n",
    "\tactual = [h.close_price for h in random_homes]\n",
    "\tpreds = []\n",
    "\tfor i in range(len(random_homes)):\n",
    "\t\tif i % 50 == 0:\n",
    "\t\t\tprint(\"on prediction {}\".format(i))\n",
    "\t\tpreds.append(single_predict(random_homes[i], homes))\n",
    "\tprint(\"done with 5000 predictions!\")\n",
    "\tprint(\"Median relative absolute error is {}\".format(mrae(preds, actual)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fcd5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['serve as the',\n",
       " 'x i −',\n",
       " 'part of the',\n",
       " 'n-gram models are',\n",
       " 'n − 1',\n",
       " '− 1 )',\n",
       " 'the language model',\n",
       " 'the probability of',\n",
       " 'can also be',\n",
       " 'also be used']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2, using wikipedia package\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "\n",
    "n_gram_article = wikipedia.page(\"n-gram\").content\n",
    "\n",
    "def get_top_k_n_grams(text,n,k):\n",
    "    word_list = text.split()\n",
    "    n_grams = []\n",
    "    freq_count = {}\n",
    "    for i in range(len(word_list)-n-1):\n",
    "        n_gram = ' '.join(word_list[i:i+n])\n",
    "        n_grams.append(n_gram)\n",
    "        if n_gram in freq_count.keys():\n",
    "            freq_count[n_gram] += 1\n",
    "        else:\n",
    "            freq_count[n_gram] = 1\n",
    "    top_n_gram_counts = sorted(freq_count.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "    return [i[0] for i in top_n_gram_counts]\n",
    "\n",
    "get_top_k_n_grams(n_gram_article,3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c795775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "[37, 39, 40, 40, 40, 40, 43, 45, 47, 48, 48, 49, 49, 50, 52, 53, 56, 56, 56, 58]\n",
      "[(48, 0), (47, 1), (49, 1), (49, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Part 3\n",
    "import numpy as np\n",
    "\n",
    "def get_k_closest(li, item_idx, k):\n",
    "\tif k >= len(li):\n",
    "\t\treturn li\n",
    "\tclosest = []\n",
    "\tfor i in range(item_idx - k, item_idx + k + 1):\n",
    "\t\tif i >= 0 and i < len(li) and i != item_idx:\n",
    "\t\t\tclosest.append((li[i], abs(li[i] - li[item_idx])))\n",
    "\treturn sorted(closest, key = lambda a: a[1])[:k]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tli = sorted([np.random.randint(100) for _ in range(100)])\n",
    "\tprint(li[44])\n",
    "\tprint(li[35:55])\n",
    "\tprint(get_k_closest(li, 44, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
