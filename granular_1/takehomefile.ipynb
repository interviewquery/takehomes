{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad70b0e",
   "metadata": {},
   "source": [
    "![logo.png](logo.png) \n",
    "\n",
    "\n",
    "## Introduction\n",
    "Welcome to Granular's Code Review Technical Challenge! \n",
    "\n",
    "This task involves commenting on a piece of code and its output. We expect it to take you at most **2 hours** of work. Your written answer should be **500-1000 words** (entries over 1000 words will be disqualified). To be clear, you will NOT be writing code, but rather reviewing it.\n",
    "\n",
    "**Disclaimer:** The code in this exercise is not representative of code quality at Granular. We are presenting code with many errors and something like this would never go into production.\n",
    "\n",
    "**Happy hunting!**\n",
    "\n",
    "## Scenario\n",
    "\n",
    "The file `toy_model_pipeline.py` contains Python code that loads historical data, trains models, and then uses those models to make predictions on a new dataset. `historical.csv` and `new_data.csv` contain train and test sets, respectively. They include simulated land sale data: each row describes a parcel of land. `y` is the sale price. `categorical_mapping.csv` maps the `category` feature to numerical features. `logfile.txt` contains output from running the script. `mse_by_model.png` visualizes the results. The goal is to predict prices as accurately as possible (interpretability is not a concern).\n",
    "\n",
    "## Your task\n",
    "\n",
    "Pretend that `toy_model_pipeline.py` is a piece of production code and review it. You should consider **architecture, style, logic, and maintainability** in your commentary. Explain bugs and other errors and give suggestions for improvement. No issue is too big or too small, and nothing is out of scope. Address any **TODO's** you see. When commenting on a specific function, or a specific line of code, please make that clear.  For example, you could structure your feedback like this:\n",
    "\n",
    "- Lines 77-79: There is a bug related to XYZ. A sentence or two of explanation.\n",
    "\n",
    "- Lines 82-88: Using parameters ABC for model DEF is a terrible idea. I would use parameter X because Y.\n",
    "\n",
    "- Lines 203-204: Brittle/copy-pasted code. Needs a function.\n",
    "\n",
    "- Overall, the code is difficult to maintain because of W. If I were responsible for this code, I'd recommend doing H.\n",
    "\n",
    "\n",
    "## Hints and further guidance\n",
    "\n",
    "This task is open-ended, and you should not feel constrained by what follows, but it may be helpful if you are unsure of where to start.\n",
    "\n",
    "The output of `toy_model_pipeline.py` is puzzling. Look at the plot of model MSEs (mean squared errors) in `mse_by_model.png`: it appears that the models do worse on new data (x=2) than they do on held-out test data (x=1). Is that just a statistical fluke? Also, it looks like the random forest does worse than the linear model. What's going on?\n",
    "\n",
    "Comment on any other interesting things you see in `mse_by_model.png` (including suggestions for improving the graph), in addition to commenting on the code itself. Your answer may include code snippets if you think that would be useful, but snippets are not required. You are free to run the code yourself as a way of exploring what it does. In case you are unable to run `toy_model_pipeline.py`, we have saved its output for you in `logfile.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c14303",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --branch granular_1 https://github.com/interviewquery/takehomes.git\n",
    "%cd takehomes/ granular_1\n",
    "!if [[ $(ls *.zip) ]]; then unzip *.zip; fi\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff91b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678452be",
   "metadata": {},
   "source": [
    "The file `toy_model_pipeline.py` is made using Python 2 and needs to be reformatted to Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2.7\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data():\n",
    "    h = pd.read_csv('historical.csv')\n",
    "    n = pd.read_csv('new_data.csv')\n",
    "    categorical_mapping = pd.read_csv('categorical_mapping.csv')\n",
    "    return h, n, categorical_mapping\n",
    "\n",
    "def train_models(data, categorical_mapping,\n",
    "                 abs_correlation_cutoff=0.15,\n",
    "                 test_fraction=0.30,\n",
    "    random_state=5123):\n",
    "    assert 0.0 < abs_correlation_cutoff < 1.0\n",
    "    assert 0.0 < test_fraction < 1.0\n",
    "    print \"training models (random forest, linear regression, mean model)\"    \n",
    "    random_forest = RandomForestRegressor(n_estimators=10,\n",
    "                                          criterion=\"mse\",\n",
    "                                          max_features=None,\n",
    "                                          max_depth=95,\n",
    "                                          bootstrap=False,\n",
    "                                          oob_score=False,\n",
    "                                          random_state=random_state,\n",
    "                                          verbose=0)\n",
    "    linear_reg = LinearRegression(fit_intercept=True)\n",
    "\n",
    "    print \"head of data:\"\n",
    "    print data.head(3)\n",
    "    print \"head of categorical mapping (mapping from category string to additional X values):\"\n",
    "    print categorical_mapping.head(3)    \n",
    "    ## Want to extend X matrix to include values in categorical_mapping.csv\n",
    "    # data = pd.concat(data, categorical_mapping)  # Get predictor variables associated with data[\"category\"]\n",
    "    # np.mean(pd.isnull(data))\n",
    "    # data.shape  # TODO Something is wrong, number of rows increased and there are lots of NAs, what's going on?\n",
    "    \n",
    "    X, y = data.drop('y', axis=1), data['y'].values.flatten()\n",
    "    print \"head of X:\"\n",
    "    print X.head(3)\n",
    "    \n",
    "    print \"selecting Xs that are correlated with y (abs correlation > {})\".format(abs_correlation_cutoff)\n",
    "    corr_y_X = data.corr()['y'][:-1]\n",
    "    predictor_indices = np.where(np.abs(corr_y_X) > abs_correlation_cutoff)[0]\n",
    "    predictor_indices_string = \", \".join([str(x) for x in predictor_indices])                                         \n",
    "    print \" selected {} predictors (columns of X): indices {}\".format(len(predictor_indices),\n",
    "                                                                      predictor_indices_string)\n",
    "        \n",
    "    X_selected = X.iloc[:, predictor_indices]\n",
    "    X_selected_train, X_selected_test, y_train, y_test = train_test_split(X_selected,\n",
    "                                                                          y,\n",
    "                                                                          test_size=test_fraction,\n",
    "                                                                          random_state=random_state)\n",
    "\n",
    "    print \"fitting random forest to training data\"\n",
    "    random_forest.fit(X=X_selected_train, y=y_train)\n",
    "    print \"random forest feature importance (top 20 predictors by importance):\"\n",
    "    importance = pd.DataFrame({\"importance\":random_forest.feature_importances_,\n",
    "                               \"predictor_index\":predictor_indices,\n",
    "                               \"predictor_name\":X.columns[predictor_indices],\n",
    "                               \"corr_with_y\":np.array(corr_y_X)[predictor_indices]})\n",
    "    print importance.sort_values(\"importance\", ascending=False).head(20)\n",
    "\n",
    "    print \"fitting linear regression to training data\"\n",
    "    linear_reg.fit(X=X_selected_train, y=y_train)\n",
    "\n",
    "    print \"fitting mean model to training data (mean of training data)\"\n",
    "    mean_model = np.mean(y_train)\n",
    "    \n",
    "    rf_predictions_train = random_forest.predict(X=X_selected_train)\n",
    "    rf_predictions_test = random_forest.predict(X=X_selected_test)\n",
    "\n",
    "    mean_predictions_train = np.ones_like(rf_predictions_train) * mean_model\n",
    "    mean_predictions_test = np.ones_like(rf_predictions_test) * mean_model\n",
    "\n",
    "    linear_predictions_train = linear_reg.predict(X=X_selected_train)\n",
    "    linear_predictions_test = linear_reg.predict(X=X_selected_test)\n",
    "\n",
    "    mse_estimates = {\"rf_train\":mean_squared_error(y_train, rf_predictions_train),\n",
    "                     \"rf_test\":mean_squared_error(y_test, rf_predictions_test),\n",
    "                     \n",
    "                     \"linear_train\":mean_squared_error(y_train, linear_predictions_train),\n",
    "                     \"linear_test\":mean_squared_error(y_test, linear_predictions_test),\n",
    "                     \n",
    "                     \"mean_model_train\":mean_squared_error(y_train, mean_predictions_train),\n",
    "                     \"mean_model_test\":mean_squared_error(y_test, mean_predictions_test)}\n",
    "\n",
    "    mse_std_errors = {\"rf_train\":np.std((y_train - rf_predictions_train) ** 2) / np.sqrt(len(y_train)),\n",
    "                      \"rf_test\":np.std((y_test - rf_predictions_test) ** 2) / np.sqrt(len(y_test)),\n",
    "                      \n",
    "                      \"linear_train\":np.std((y_train - linear_predictions_train) ** 2) / np.sqrt(len(y_train)),\n",
    "                      \"linear_test\":np.std((y_test - linear_predictions_test) ** 2) / np.sqrt(len(y_test)),\n",
    "                      \n",
    "                      \"mean_model_train\":np.std((y_train - mean_predictions_train) ** 2) / np.sqrt(len(y_train)),\n",
    "                      \"mean_model_test\":np.std((y_test - mean_predictions_test) ** 2) / np.sqrt(len(y_test))}\n",
    "           \n",
    "    return random_forest, linear_reg, mean_model, predictor_indices, mse_estimates, mse_std_errors\n",
    "\n",
    "def get_predictions(random_forest, linear_reg, mean_model, predictor_indices, data):\n",
    "    X_selected = data.iloc[:, predictor_indices]\n",
    "    rf_predictions = random_forest.predict(X_selected)\n",
    "    linear_predictions = linear_reg.predict(X_selected)\n",
    "    mean_model_predictions = mean_model * np.ones_like(rf_predictions)\n",
    "    return (rf_predictions, linear_predictions, mean_model_predictions)\n",
    "\n",
    "def save_plot_mse_estimates(mse_estimates, mse_std_errors,\n",
    "    outfile=\"mse_by_model.png\"):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    mse_train_test_new = {\"random forest\":[mse_estimates[\"rf_train\"],\n",
    "                                           mse_estimates[\"rf_test\"],\n",
    "                                           mse_estimates[\"rf_new_data\"]],\n",
    "                          \"linear model\":[mse_estimates[\"linear_train\"],\n",
    "                                          mse_estimates[\"linear_test\"],\n",
    "                                          mse_estimates[\"linear_new_data\"]],\n",
    "                          \"mean model\":[mse_estimates[\"mean_model_train\"],\n",
    "                                        mse_estimates[\"mean_model_test\"],\n",
    "                                        mse_estimates[\"mean_model_new_data\"]]}\n",
    "    std_errors = {\"random forest\":[mse_std_errors[\"rf_train\"],\n",
    "                                   mse_std_errors[\"rf_test\"],\n",
    "                                   mse_std_errors[\"rf_new_data\"]],\n",
    "                  \"linear model\":[mse_std_errors[\"linear_train\"],\n",
    "                                  mse_std_errors[\"linear_test\"],\n",
    "                                  mse_std_errors[\"linear_new_data\"]],\n",
    "                  \"mean model\":[mse_std_errors[\"mean_model_train\"],\n",
    "                                mse_std_errors[\"mean_model_test\"],\n",
    "                                mse_std_errors[\"mean_model_new_data\"]]}\n",
    "    for model, errors in mse_train_test_new.iteritems():\n",
    "        plt.plot(range(len(errors)), errors, \"-o\", label=model, linewidth=1.5, ms=9.0)\n",
    "        ax.errorbar(range(len(errors)), errors, yerr=std_errors[model], ls='none')\n",
    "\n",
    "    plt.xlim(-0.2, len(errors) - 1 + 0.2)\n",
    "    plt.xlabel(\"0 = train, 1 = test, 2 = new data\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"Error Estimates (Mean Squared Error) by Model\\nWith 95% Confidence Intervals\")\n",
    "    ax.grid(color='grey', linestyle='--', alpha=0.50)\n",
    "    plt.legend(loc=4)\n",
    "    plt.savefig(outfile)\n",
    "    plt.close(\"all\")\n",
    "    return\n",
    "    \n",
    "def main():\n",
    "    historical_data, new_data, categorical_mapping = load_data()\n",
    "    n_obs, n_features = historical_data.drop('y', axis=1).shape\n",
    "    print \"Training data: {} observations of {} predictors.\".format(n_obs, n_features)\n",
    "    \n",
    "    (random_forest,\n",
    "     linear_reg,\n",
    "     mean_model,\n",
    "     predictor_indices,\n",
    "     mse_estimates,\n",
    "     mse_std_errors) = train_models(historical_data, categorical_mapping,\n",
    "                                    test_fraction=0.25, abs_correlation_cutoff=0.10)\n",
    "\n",
    "    (rf_predictions,\n",
    "     linear_predictions,\n",
    "     mean_model_predictions) = get_predictions(random_forest, linear_reg, mean_model,\n",
    "                                               predictor_indices, new_data.drop('y', axis=1))\n",
    "\n",
    "    mse_estimates[\"rf_new_data\"] = mean_squared_error(new_data[\"y\"], rf_predictions)\n",
    "    mse_estimates[\"linear_new_data\"] = mean_squared_error(new_data[\"y\"], linear_predictions)\n",
    "    mse_estimates[\"mean_model_new_data\"] = mean_squared_error(new_data[\"y\"], mean_model_predictions)\n",
    "\n",
    "    sqrt_len_new_y = np.sqrt(len(new_data[\"y\"]))\n",
    "    mse_std_errors[\"rf_new_data\"] = np.std((new_data[\"y\"] - rf_predictions) ** 2) / sqrt_len_new_y\n",
    "    mse_std_errors[\"linear_new_data\"] = np.std((new_data[\"y\"] - linear_predictions) ** 2) / sqrt_len_new_y\n",
    "    mse_std_errors[\"mean_model_new_data\"] = np.std((new_data[\"y\"] - mean_model_predictions) ** 2) / sqrt_len_new_y\n",
    "    \n",
    "    corr_y_rf_prediction = np.corrcoef(new_data[\"y\"], rf_predictions)[0, 1]\n",
    "    print \"corr(y, rf_predictions) on new data: {}\".format(corr_y_rf_prediction)\n",
    "    corr_y_linear_prediction = np.corrcoef(new_data[\"y\"], linear_predictions)[0, 1]\n",
    "    print \"corr(y, linear_predictions) on new data: {}\".format(corr_y_linear_prediction)       \n",
    "\n",
    "    print \"error estimates by model:\"\n",
    "    print \" linear regression: train {}, test {}, new data {}\".format(mse_estimates[\"linear_train\"],\n",
    "                                                                      mse_estimates[\"linear_test\"],\n",
    "                                                                      mse_estimates[\"linear_new_data\"])\n",
    "    print \" random forest: train {}, test {}, new data {}\".format(mse_estimates[\"rf_train\"],\n",
    "                                                                  mse_estimates[\"rf_test\"],\n",
    "                                                                  mse_estimates[\"rf_new_data\"])\n",
    "    print \" mean model : train {}, test {}, new data {}\".format(mse_estimates[\"mean_model_train\"],\n",
    "                                                                mse_estimates[\"mean_model_test\"],\n",
    "                                                                mse_estimates[\"mean_model_new_data\"])\n",
    "\n",
    "    save_plot_mse_estimates(mse_estimates, mse_std_errors)\n",
    "    \n",
    "    print \"all done\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
